# ──────────────────────────────────────────────
# AuRE – environment configuration
# Copy this file to .env and fill in the values.
# ──────────────────────────────────────────────

# ── LLM provider ────────────────────────────
# Supported: "openai", "gemini", "local" (Ollama / LM Studio / vLLM)
# If unset, AuRE auto-detects from whichever API key is present.
LLM_PROVIDER=openai

# ── API key ─────────────────────────────────
# Generic key (works for any provider):
LLM_API_KEY=sk-...
# Or use the provider-specific variable instead:
# OPENAI_API_KEY=sk-...
# GEMINI_API_KEY=...

# ── Model name ──────────────────────────────
# Defaults: openai → gpt-4o-mini, gemini → gemini-2.0-flash-lite, local → llama3
LLM_MODEL=gpt-4o-mini

# ── Base URL (local / OpenAI-compatible only) ─
# Uncomment and set when using Ollama, LM Studio, vLLM, etc.
# LLM_BASE_URL=http://localhost:11434/v1

# ── Generation settings ─────────────────────
LLM_TEMPERATURE=0.0
LLM_TIMEOUT=120          # seconds to wait per LLM call

# ── Fitting ──────────────────────────────────
# Method: "lm" (Levenberg-Marquardt, fast), "de" (Differential Evolution),
#         "dream" (MCMC with uncertainty quantification)
FIT_METHOD=dream
FIT_STEPS=1000             # number of MCMC / optimizer steps
FIT_BURN=1000              # burn-in steps (DREAM only)

# ── LangSmith tracing (optional) ────────────
# Set to "true" to send traces to LangSmith for debugging / monitoring.
# Requires the langsmith package (pip install langsmith).
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=ls-...
# LANGCHAIN_PROJECT=aure
